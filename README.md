<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 13bec14 (Initial commit - Web Scraping ETL Pipeline)
# ğŸ› ï¸ Web Scraping ETL Pipeline  

This project is a simple **ETL (Extract, Transform, Load) pipeline** that scrapes quotes from [Quotes to Scrape](https://quotes.toscrape.com), cleans the data, and saves it to a CSV file.  

---

## ğŸ“Œ Features  
âœ… **Extracts** quotes and author names using web scraping (**BeautifulSoup**).  
âœ… **Cleans and standardizes** data using **Pandas**.  
âœ… **Saves** transformed data as a **CSV file** for further use.  

---

## ğŸ—ï¸ Project Structure  

```
ğŸ“‚ Web Scraping ETL Pipeline
â”‚â”€â”€ ğŸ“‚ scripts
â”‚   â”œâ”€â”€ extract.py        # Extracts data from the website
â”‚   â”œâ”€â”€ transform.py      # Cleans and standardizes the data
â”‚   â”œâ”€â”€ load.py           # Saves the data to a CSV file
â”‚â”€â”€ ğŸ“„ main.py            # Runs the complete ETL process
â”‚â”€â”€ ğŸ“„ requirements.txt   # List of dependencies
â”‚â”€â”€ ğŸ“„ README.md          # Project documentation
```
## ğŸš€ Installation & Setup
1ï¸âƒ£ Clone the Repository
```
git clone https://github.com/Gajoshana2910/Web-Scraping-ETL-Pipeline.git
cd Web-Scraping-ETL-Pipeline
```
2ï¸âƒ£ Set Up a Virtual Environment (Optional, Recommended)
```
python -m venv venv
source venv/bin/activate  # On macOS/Linux
venv\Scripts\activate     # On Windows
```
3ï¸âƒ£ Install Dependencies
```
pip install -r requirements.txt
```
ğŸ”„ Running the ETL Pipeline
Run the main script to execute the entire ETL process:
```
python main.py
```
## ğŸ“Œ Sample Output
```
[INFO] Starting ETL process...
[INFO] Extraction successful!
[INFO] Transformation successful!
[INFO] Data saved to data/scraped_data.csv
[INFO] ETL process completed successfully! ğŸš€
```
## ğŸ“œ Code Overview

1ï¸âƒ£ Extract Data (scripts/extract.py)

- Scrapes quotes and authors from the website.
- Handles missing author names.
- Stores extracted data in a Pandas DataFrame.

2ï¸âƒ£ Transform Data (scripts/transform.py)

- Cleans special characters from quotes.
- Capitalizes author names.

3ï¸âƒ£ Load Data (scripts/load.py)

- Saves the cleaned data as data/scraped_data.csv.

## ğŸ“‚ Output File

After running the ETL pipeline, the cleaned data will be saved as:
```
ğŸ“‚ data
   â”œâ”€â”€ scraped_data.csv
```

## ğŸ“¦ Dependencies
This project requires the following Python libraries:
```
beautifulsoup4
pandas
requests
```
You can install them using:
```
pip install -r requirements.txt
```

## ğŸ› ï¸ Next Steps / Improvements
```
âœ… Add Airflow/Docker for automation.
âœ… Store data in a database (PostgreSQL, Snowflake, etc.) instead of CSV.
âœ… Scrape multiple pages instead of just the first one.
```
## ğŸ“œ License

This project is licensed under the MIT License 
ğŸ‘‰ see the [LICENSE](https://github.com/Gajoshana2910/Web-Scraping-ETL-Pipeline/blob/main/LICENSE) file for details.  

## ğŸ‘¨â€ğŸ’» Developed by

ğŸ‘‰ [Gajalakshmi A K](https://github.com/Gajoshana2910)

<<<<<<< HEAD
=======
=======
 
>>>>>>> 081b8f6 (Initial commit - Web Scraping ETL Pipeline)
>>>>>>> 13bec14 (Initial commit - Web Scraping ETL Pipeline)
